{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sequencing_tools.fastq_tools import reverse_complement, onehot_sequence_encoder\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.bed_utils import data_generator\n",
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from torch.autograd.profiler import profile\n",
    "\n",
    "acceptable_nuc = list('ACTGN')\n",
    "dna_encoder = onehot_sequence_encoder(''.join(acceptable_nuc))\n",
    "\n",
    "def generate_seq(n = 100):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        seq = [random.choice(list('ACTG')) for i in range(400)]\n",
    "        seq = ''.join(seq)\n",
    "        label = random.choice([0,1])\n",
    "        X.append(dna_encoder.transform(seq).transpose())\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Deep_cfNA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep_cfNA, self).__init__()\n",
    "        self.conv_1d = nn.Conv1d(in_channels=len(acceptable_nuc),\n",
    "                                 out_channels=160,\n",
    "                                kernel_size=26,\n",
    "                                stride=1)\n",
    "        self.LSTM = nn.LSTM(input_size=26, hidden_size=64,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(4160, 50)\n",
    "        self.linear2 = nn.Linear(50, 25)\n",
    "        self.linear3 = nn.Linear(25, 1)\n",
    "        \n",
    "    def initialize_weight(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv_1d(x)\n",
    "        y = F.relu(y)\n",
    "        y = F.max_pool1d(y, kernel_size=50, stride=13)\n",
    "        batch_size = y.size(0)\n",
    "        y = y.view(batch_size, -1)\n",
    "        y = F.dropout(y, p = 0.2)\n",
    "        #y, (hidden, cell_state)= self.LSTM(y)\n",
    "        #y = y[:, -1]\n",
    "        y = F.dropout(y, p = 0.5)\n",
    "        y = self.linear1(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.linear3(y)\n",
    "        y = F.sigmoid(y)\n",
    "        return y\n",
    "    \n",
    "    def lstm_layer(self, x):\n",
    "        y = self.conv_1d(x)\n",
    "        #print('conv1d:', y.shape)\n",
    "        y = F.max_pool1d(F.relu(y), kernel_size=50, stride=13)\n",
    "        #print('max pool:', y.shape)\n",
    "        y = F.dropout(y, p = 0.2)\n",
    "        y, (hidden, cell_state)= self.LSTM(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "work_dir = '/stor/work/Lambowitz/cdw2854/cell_Free_nucleotides/tgirt_map/classifier'\n",
    "DNA_bed = work_dir + '/train_DNA.bed'\n",
    "RNA_bed = work_dir + '/train_RNA.bed'\n",
    "fa = '/stor/work/Lambowitz/ref/hg19/genome/hg19_genome.fa'\n",
    "batch = 500\n",
    "\n",
    "gen_data = data_generator(RNA_bed, DNA_bed, fa, batch_size=batch, seed = 1, N_padded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/200] loss: 0.246, used 1.625 sec\n",
      "[40/200] loss: 0.194, used 1.489 sec\n",
      "[60/200] loss: 0.177, used 1.584 sec\n",
      "[80/200] loss: 0.126, used 1.536 sec\n",
      "[100/200] loss: 0.141, used 1.557 sec\n",
      "[120/200] loss: 0.104, used 1.616 sec\n",
      "[140/200] loss: 0.099, used 1.777 sec\n",
      "[160/200] loss: 0.137, used 1.608 sec\n",
      "[180/200] loss: 0.124, used 1.440 sec\n",
      "[200/200] loss: 0.111, used 1.526 sec\n"
     ]
    }
   ],
   "source": [
    "model = Deep_cfNA()\n",
    "model.initialize_weight()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "losses = []\n",
    "steps = 200\n",
    "\n",
    "for step in range(steps):\n",
    "    start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    X,y = next(gen_data)\n",
    "    X.requires_grad_()\n",
    "    pred_y = model(X)\n",
    "    loss = F.binary_cross_entropy(pred_y.view(-1), y)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end = time.time()\n",
    "    if (step+1) % 20 == 0:\n",
    "        print('[{step}/{steps}] loss: {loss}, used {time} sec'\\\n",
    "                .format(step = step + 1, \n",
    "                        steps=steps, \n",
    "                        loss = '%.3f' %loss.item(), \n",
    "                        time = '%.3f' %(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbc21a69b00>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stor/work/Lambowitz/cdw2854/src/miniconda3/envs/tensorflow/lib/python3.6/site-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 5, 400])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Linear in module torch.nn.modules.linear:\n",
      "\n",
      "class Linear(torch.nn.modules.module.Module)\n",
      " |  Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      " |  \n",
      " |  Args:\n",
      " |      in_features: size of each input sample\n",
      " |      out_features: size of each output sample\n",
      " |      bias: If set to False, the layer will not learn an additive bias.\n",
      " |          Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, *, in\\_features)` where :math:`*` means any number of\n",
      " |        additional dimensions\n",
      " |      - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
      " |        are the same shape as the input.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight: the learnable weights of the module of shape\n",
      " |          `(out_features x in_features)`\n",
      " |      bias:   the learnable bias of the module of shape `(out_features)`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Linear(20, 30)\n",
      " |      >>> input = torch.randn(128, 20)\n",
      " |      >>> output = m(input)\n",
      " |      >>> print(output.size())\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Linear\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_features, out_features, bias=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |                  print(m)\n",
      " |                  if type(m) == nn.Linear:\n",
      " |                      m.weight.data.fill_(1.0)\n",
      " |                      print(m.weight)\n",
      " |      \n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device)\n",
      " |      \n",
      " |      .. function:: to(dtype)\n",
      " |      \n",
      " |      .. function:: to(device, dtype)\n",
      " |      \n",
      " |      It has similar signature as :meth:`torch.Tensor.to`, but does not take\n",
      " |      a Tensor and only takes in floating point :attr:`dtype` s. In\n",
      " |      particular, this method will only cast the floating point parameters and\n",
      " |      buffers to :attr:`dtype`. It will still move the integral parameters and\n",
      " |      buffers to :attr:`device`, if that is given. See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
